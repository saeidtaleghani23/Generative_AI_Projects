{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 02:38:22.530029: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-09 02:38:22.532106: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-09 02:38:22.565165: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-09 02:38:22.565195: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-09 02:38:22.566116: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-09 02:38:22.572606: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-09 02:38:22.573573: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-09 02:38:23.157185: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, LeakyReLU, Activation, Concatenate, Dropout\n",
    "#from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "#from instancenormalization import InstanceNormalization\n",
    "from tensorflow.keras.layers import Input, UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from utils import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code has been downloaded from \n",
    "# https://github.com/keras-team/keras-contrib/blob/master/keras_contrib/layers/normalization/instancenormalization.py\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "class InstanceNormalization(Layer):\n",
    "    \"\"\"Instance normalization layer.\n",
    "\n",
    "    Normalize the activations of the previous layer at each step,\n",
    "    i.e. applies a transformation that maintains the mean activation\n",
    "    close to 0 and the activation standard deviation close to 1.\n",
    "\n",
    "    # Arguments\n",
    "        axis: Integer, the axis that should be normalized\n",
    "            (typically the features axis).\n",
    "            For instance, after a `Conv2D` layer with\n",
    "            `data_format=\"channels_first\"`,\n",
    "            set `axis=1` in `InstanceNormalization`.\n",
    "            Setting `axis=None` will normalize all values in each\n",
    "            instance of the batch.\n",
    "            Axis 0 is the batch dimension. `axis` cannot be set to 0 to avoid errors.\n",
    "        epsilon: Small float added to variance to avoid dividing by zero.\n",
    "        center: If True, add offset of `beta` to normalized tensor.\n",
    "            If False, `beta` is ignored.\n",
    "        scale: If True, multiply by `gamma`.\n",
    "            If False, `gamma` is not used.\n",
    "            When the next layer is linear (also e.g. `nn.relu`),\n",
    "            this can be disabled since the scaling\n",
    "            will be done by the next layer.\n",
    "        beta_initializer: Initializer for the beta weight.\n",
    "        gamma_initializer: Initializer for the gamma weight.\n",
    "        beta_regularizer: Optional regularizer for the beta weight.\n",
    "        gamma_regularizer: Optional regularizer for the gamma weight.\n",
    "        beta_constraint: Optional constraint for the beta weight.\n",
    "        gamma_constraint: Optional constraint for the gamma weight.\n",
    "\n",
    "    # Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a Sequential model.\n",
    "\n",
    "    # Output shape\n",
    "        Same shape as input.\n",
    "\n",
    "    # References\n",
    "        - [Layer Normalization](https://arxiv.org/abs/1607.06450)\n",
    "        - [Instance Normalization: The Missing Ingredient for Fast Stylization](\n",
    "        https://arxiv.org/abs/1607.08022)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 axis=None,\n",
    "                 epsilon=1e-3,\n",
    "                 center=True,\n",
    "                 scale=True,\n",
    "                 beta_initializer='zeros',\n",
    "                 gamma_initializer='ones',\n",
    "                 beta_regularizer=None,\n",
    "                 gamma_regularizer=None,\n",
    "                 beta_constraint=None,\n",
    "                 gamma_constraint=None,\n",
    "                 **kwargs):\n",
    "        super(InstanceNormalization, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.axis = axis\n",
    "        self.epsilon = epsilon\n",
    "        self.center = center\n",
    "        self.scale = scale\n",
    "        self.beta_initializer = initializers.get(beta_initializer)\n",
    "        self.gamma_initializer = initializers.get(gamma_initializer)\n",
    "        self.beta_regularizer = regularizers.get(beta_regularizer)\n",
    "        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n",
    "        self.beta_constraint = constraints.get(beta_constraint)\n",
    "        self.gamma_constraint = constraints.get(gamma_constraint)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        ndim = len(input_shape)\n",
    "        if self.axis == 0:\n",
    "            raise ValueError('Axis cannot be zero')\n",
    "\n",
    "        if (self.axis is not None) and (ndim == 2):\n",
    "            raise ValueError('Cannot specify axis for rank 1 tensor')\n",
    "\n",
    "        self.input_spec = InputSpec(ndim=ndim)\n",
    "\n",
    "        if self.axis is None:\n",
    "            shape = (1,)\n",
    "        else:\n",
    "            shape = (input_shape[self.axis],)\n",
    "\n",
    "        if self.scale:\n",
    "            self.gamma = self.add_weight(shape=shape,\n",
    "                                         name='gamma',\n",
    "                                         initializer=self.gamma_initializer,\n",
    "                                         regularizer=self.gamma_regularizer,\n",
    "                                         constraint=self.gamma_constraint)\n",
    "        else:\n",
    "            self.gamma = None\n",
    "        if self.center:\n",
    "            self.beta = self.add_weight(shape=shape,\n",
    "                                        name='beta',\n",
    "                                        initializer=self.beta_initializer,\n",
    "                                        regularizer=self.beta_regularizer,\n",
    "                                        constraint=self.beta_constraint)\n",
    "        else:\n",
    "            self.beta = None\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        input_shape = K.int_shape(inputs)\n",
    "        reduction_axes = list(range(0, len(input_shape)))\n",
    "\n",
    "        if self.axis is not None:\n",
    "            del reduction_axes[self.axis]\n",
    "\n",
    "        del reduction_axes[0]\n",
    "\n",
    "        mean = K.mean(inputs, reduction_axes, keepdims=True)\n",
    "        stddev = K.std(inputs, reduction_axes, keepdims=True) + self.epsilon\n",
    "        normed = (inputs - mean) / stddev\n",
    "\n",
    "        broadcast_shape = [1] * len(input_shape)\n",
    "        if self.axis is not None:\n",
    "            broadcast_shape[self.axis] = input_shape[self.axis]\n",
    "\n",
    "        if self.scale:\n",
    "            broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n",
    "            normed = normed * broadcast_gamma\n",
    "        if self.center:\n",
    "            broadcast_beta = K.reshape(self.beta, broadcast_shape)\n",
    "            normed = normed + broadcast_beta\n",
    "        return normed\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'axis': self.axis,\n",
    "            'epsilon': self.epsilon,\n",
    "            'center': self.center,\n",
    "            'scale': self.scale,\n",
    "            'beta_initializer': initializers.serialize(self.beta_initializer),\n",
    "            'gamma_initializer': initializers.serialize(self.gamma_initializer),\n",
    "            'beta_regularizer': regularizers.serialize(self.beta_regularizer),\n",
    "            'gamma_regularizer': regularizers.serialize(self.gamma_regularizer),\n",
    "            'beta_constraint': constraints.serialize(self.beta_constraint),\n",
    "            'gamma_constraint': constraints.serialize(self.gamma_constraint)\n",
    "        }\n",
    "        base_config = super(InstanceNormalization, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://efrosgans.eecs.berkeley.edu/cyclegan/datasets/vangogh2photo.zip\n",
      "206086144/306590349 [===================>..........] - ETA: 1:13"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "#dataset_name = 'apple2orange'  # Other options: 'summer2winter_yosemite', 'horse2zebra', 'vangogh2photo'.\n",
    "dataset_name = 'vangogh2photo'\n",
    "DOWNLOAD_URL = f'http://efrosgans.eecs.berkeley.edu/cyclegan/datasets/{dataset_name}.zip'\n",
    "\n",
    "# Specify your existing download directory\n",
    "#data_directory = os.path.abspath(f'./datasets/{dataset_name}')  # Convert to absolute path\n",
    "data_directory = os.path.abspath(f'./datasets')  # Convert to absolute path\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(data_directory, exist_ok=True)\n",
    "\n",
    "zip_file_path = os.path.join(data_directory, f'{dataset_name}.zip')  # Path for the zip file\n",
    "\n",
    "# Download the dataset directly to your specified directory\n",
    "dataset_path = tf.keras.utils.get_file(\n",
    "    fname=zip_file_path,  # Save it directly in the desired directory\n",
    "    origin=DOWNLOAD_URL,\n",
    "    cache_dir=None,  # Do not use cache_dir to prevent nested folders\n",
    "    extract=False\n",
    ")\n",
    "\n",
    "# Manually unzip the dataset\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(data_directory)\n",
    "print(f'Dataset downloaded and extracted to: {dataset_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# required function in defining Generator and Discriminator models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Down sample fucntion\n",
    "def downsampling(in_layer: tf.Tensor, num_filters: int, kernel_size: int = 4, strides: int = 2) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Downsamples an input tensor using a Conv2D layer, followed by LeakyReLU activation and \n",
    "    InstanceNormalization.\n",
    "\n",
    "    Args:\n",
    "        in_layer (tf.Tensor): Input tensor to be downsampled.\n",
    "        num_filters (int): Number of filters for the Conv2D layer.\n",
    "        kernel_size (int, optional): Size of the convolutional kernel. Defaults to 4.\n",
    "        strides (int, optional): Stride size for the convolution operation. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: The downsampled output tensor after applying convolution, activation, and normalization.\n",
    "    \"\"\"\n",
    "    downsampled = Conv2D(num_filters, kernel_size=kernel_size, strides=strides, padding='same')(in_layer)\n",
    "    downsampled = LeakyReLU(alpha=0.2)(downsampled)\n",
    "    downsampled = InstanceNormalization()(downsampled)\n",
    "    return downsampled\n",
    "\n",
    "# Up sample function\n",
    "def upsampling(in_layer: tf.Tensor, skip_layer: tf.Tensor, num_filters: int, kernel_size: int = 4, strides: int = 1, dropout_rate: float = 0) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Upsamples an input tensor using UpSampling2D and Conv2D layers, with optional dropout and \n",
    "    InstanceNormalization, followed by concatenation with a skip connection.\n",
    "\n",
    "    Args:\n",
    "        in_layer (tf.Tensor): Input tensor to be upsampled.\n",
    "        skip_layer (tf.Tensor): Tensor to concatenate as a skip connection with the upsampled tensor.\n",
    "        num_filters (int): Number of filters for the Conv2D layer.\n",
    "        kernel_size (int, optional): Size of the convolutional kernel. Defaults to 4.\n",
    "        strides (int, optional): Stride size for the convolution operation. Defaults to 1.\n",
    "        dropout_rate (float, optional): Dropout rate (0 means no dropout). Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: The upsampled output tensor after applying convolution, normalization, and concatenation.\n",
    "    \"\"\"\n",
    "    upsampled = UpSampling2D(size=2)(in_layer)\n",
    "    upsampled = Conv2D(num_filters, kernel_size=kernel_size, strides=strides, padding='same', activation='relu')(upsampled)\n",
    "    if dropout_rate:\n",
    "        upsampled = Dropout(dropout_rate)(upsampled)\n",
    "    upsampled = InstanceNormalization()(upsampled)\n",
    "    upsampled = Concatenate()([upsampled, skip_layer])\n",
    "    return upsampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define U-Net shape generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(img_shape: tuple, in_channels: int = 3, num_filters: int = 32) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Builds a U-Net style generator model with downsampling and upsampling layers, often used for \n",
    "    image generation tasks.\n",
    "\n",
    "    Args:\n",
    "        img_shape (tuple): Shape of the input image (height, width, channels).\n",
    "        in_channels (int, optional): Number of channels in the output image. Defaults to 3.\n",
    "        num_filters (int, optional): Base number of filters for the downsampling layers. Defaults to 32.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: The generator model built with U-Net architecture.\n",
    "    \"\"\"\n",
    "    # image shape\n",
    "    input_layer = Input(shape=img_shape)\n",
    "    \n",
    "    # downsampling in U-Net model\n",
    "    down_sample_1 = downsampling(in_layer=input_layer, num_filters=num_filters)\n",
    "    down_sample_2 = downsampling(in_layer=down_sample_1, num_filters=2 * num_filters)\n",
    "    down_sample_3 = downsampling(in_layer=down_sample_2, num_filters=4 * num_filters)\n",
    "    bottleneck = downsampling(in_layer=down_sample_3, num_filters=8 * num_filters)\n",
    "    \n",
    "    # upsampling in U-Net model\n",
    "    upsample_1 = upsampling(in_layer=bottleneck, skip_layer=down_sample_3, num_filters=4 * num_filters)\n",
    "    upsample_2 = upsampling(in_layer=upsample_1, skip_layer=down_sample_2, num_filters=2 * num_filters)\n",
    "    upsample_3 = upsampling(in_layer=upsample_2, skip_layer=down_sample_1, num_filters=num_filters)\n",
    "    upsample_4 = UpSampling2D(size=2)(upsample_3)\n",
    "    \n",
    "    # output layer\n",
    "    output_img = Conv2D(in_channels, kernel_size=4, strides=1, padding='same', activation='tanh')(upsample_4)\n",
    "    \n",
    "    # return the generative model\n",
    "    return Model(input_layer, output_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define the discriminator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disc_block(in_layer: tf.Tensor, num_filters: int, kernel_size: int = 4, instance_normalization: bool = True) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Builds a convolutional block with Conv2D, LeakyReLU activation, and optional InstanceNormalization, \n",
    "    commonly used in discriminator networks.\n",
    "\n",
    "    Args:\n",
    "        in_layer (tf.Tensor): Input tensor for the block.\n",
    "        num_filters (int): Number of filters for the Conv2D layer.\n",
    "        kernel_size (int, optional): Size of the convolutional kernel. Defaults to 4.\n",
    "        instance_normalization (bool, optional): Whether to apply InstanceNormalization. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: The output tensor after applying convolution, activation, and optional normalization.\n",
    "    \"\"\"\n",
    "    disc_layer = Conv2D(num_filters, kernel_size=kernel_size, strides=2, padding='same')(in_layer)\n",
    "    disc_layer = LeakyReLU(alpha=0.2)(disc_layer)\n",
    "    if instance_normalization:\n",
    "        disc_layer = InstanceNormalization()(disc_layer)\n",
    "    return disc_layer\n",
    "\n",
    "def build_discriminator(img_shape: tuple, num_filters: int = 64) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Builds a discriminator model using multiple convolutional blocks and outputs a single-channel \n",
    "    feature map. The model uses a sequence of downsampling layers with increasing filter sizes.\n",
    "\n",
    "    Args:\n",
    "        img_shape (tuple): Shape of the input image (height, width, channels).\n",
    "        num_filters (int, optional): Base number of filters for the first convolutional block. Defaults to 64.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: The discriminator model built for distinguishing between real and generated images.\n",
    "    \"\"\"\n",
    "    input_layer = Input(shape=img_shape)\n",
    "    \n",
    "    # First block, without instance normalization\n",
    "    disc_block_1 = disc_block(input_layer, num_filters=num_filters, instance_normalization=False)\n",
    "    \n",
    "    # Subsequent blocks with increasing filters\n",
    "    disc_block_2 = disc_block(disc_block_1, num_filters * 2)\n",
    "    disc_block_3 = disc_block(disc_block_2, num_filters * 4)\n",
    "    disc_block_4 = disc_block(disc_block_3, num_filters * 8)\n",
    "    \n",
    "    # Final output layer\n",
    "    disc_output = Conv2D(1, kernel_size=4, strides=1, padding='same')(disc_block_4)\n",
    "    \n",
    "    # Return the discriminator model\n",
    "    return Model(input_layer, disc_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_filter = 32\n",
    "discriminator_filters = 64\n",
    "# image shape\n",
    "image_height = 128\n",
    "image_width = 128\n",
    "# input shape\n",
    "channels = 3\n",
    "input_shape = (image_height, image_width, channels)\n",
    "# loss weights\n",
    "lambda_cycle = 10.0\n",
    "lambda_identity = 0.1 * lambda_cycle\n",
    "# optimizer\n",
    "optimizer = Adam (learning_rate= 0.0002, beta_1= 0.5)\n",
    "\n",
    "patch = int (image_height / 2**4)\n",
    "patch_gan_shape = (patch, patch, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CycleGAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator models \n",
    "disc_A = build_discriminator(img_shape = input_shape, num_filters = discriminator_filters)\n",
    "disc_A.compile(loss = 'mse',\n",
    "optimizer = optimizer,\n",
    "metrics = ['accuracy'])\n",
    "\n",
    "disc_B = build_discriminator(img_shape = input_shape, num_filters = discriminator_filters)\n",
    "disc_B.compile(loss = 'mse',\n",
    "optimizer = optimizer,\n",
    "metrics = ['accuracy'])\n",
    "\n",
    "# generators model \n",
    "gen_AtoB = build_generator(img_shape = input_shape, in_channels = channels, num_filters = generator_filter)\n",
    "gen_BtoA = build_generator(img_shape = input_shape, in_channels = channels, num_filters = generator_filter)\n",
    "\n",
    "#CycleGAN model\n",
    "real_image_A = Input(shape=input_shape)\n",
    "real_image_B = Input(shape=input_shape)\n",
    "# generate fake samples from both generators\n",
    "fake_image_B = gen_AtoB(real_image_A)\n",
    "fake_image_A = gen_BtoA(real_image_B)\n",
    "\n",
    "# *****Reconstruction Loss*****\n",
    "# reconstruct original samples from both generators using fake images \n",
    "reconstruct_A = gen_BtoA(fake_image_B) # it must be similar to real images from domain A\n",
    "reconstruct_B = gen_AtoB(fake_image_A) # it must be similar to real images from domain B\n",
    "\n",
    "# *****Identity Loss*****\n",
    "# generate identity samples\n",
    "identity_A = gen_BtoA(real_image_A) # it must be equal to real image from domain A\n",
    "identity_B = gen_AtoB(real_image_B) # it must be equal to real image from domain B\n",
    "# disable discriminator training\n",
    "disc_A.trainable = False\n",
    "disc_B.trainable = False\n",
    "\n",
    "# *****Adversarial Loss*****\n",
    "# use discriminator to classify real vs fake \n",
    "output_A = disc_A(fake_image_A)\n",
    "output_B = disc_B(fake_image_B)\n",
    "# Combined model trains generators to fool discriminators to fool discriminators\n",
    "cycle_gan = Model(inputs= [real_image_A, real_image_B],\n",
    "            outputs = [output_A, output_B, reconstruct_A, reconstruct_B, identity_A, identity_B])\n",
    "\n",
    "cycle_gan.compile (loss = ['mse', 'mse', 'mae', 'mae', 'mae', 'mae'], # mse  is used for Adversarial losses while mae is used for identity and reconstruction losses\n",
    "             loss_weights = [1, 1, lambda_cycle, lambda_cycle, lambda_identity, lambda_identity], # how losses are combined to get final loss value\n",
    "             optimizer= optimizer # which optimizer is used\n",
    "             ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training CycleGAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainig(gen_AtoB,\n",
    "                gen_BtoA, \n",
    "                disc_A, \n",
    "                disc_B, \n",
    "                cyclegan, \n",
    "                patch_gan_shape, \n",
    "                epochs,\n",
    "                path= './datasets/{}'.format(dataset_name),\n",
    "                batch_size = 1, \n",
    "                sample_interval = 50):\n",
    "    # Adversarial loss ground truths\n",
    "    real_labels = np.ones((batch_size,) + patch_gan_shape)\n",
    "    fake_labels = np.zeros((batch_size,) + patch_gan_shape)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch={epoch}')\n",
    "        for idx, (imgs_A, imgs_B) in enumerate(batch_generator(path, batch_size, image_res=[image_height, image_width])) :\n",
    "            # generate fake samples from both generators\n",
    "            fake_B = gen_AtoB.predict(imgs_A)\n",
    "            fake_A = gen_BtoA.predict(imgs_B)\n",
    "            \n",
    "            # Train discriminators\n",
    "            disc_A_loss_real = disc_A.train_on_batch(imgs_A, real_labels)\n",
    "            disc_A_loss_fake = disc_A.train_on_batch(fake_A, fake_labels)\n",
    "            disc_A_loss = 0.5 * np.add(disc_A_loss_real, disc_A_loss_fake)\n",
    "            \n",
    "            disc_B_loss_real =  disc_B.train_on_batch(imgs_B, real_labels)\n",
    "            disc_B_loss_fake = disc_B.train_on_batch(fake_B, fake_labels)\n",
    "            disc_B_loss = 0.5 * np.add(disc_B_loss_real, disc_B_loss_fake)\n",
    "            # total discriminator loss\n",
    "            discriminator_loss = 0.5 * np.add(disc_A_loss, disc_B_loss)\n",
    "            \n",
    "            # Train generator\n",
    "            gen_loss = cycle_gan.train_on_batch([imgs_A, imgs_B],\n",
    "                                                [\n",
    "                                                 real_labels, real_labels, \n",
    "                                                 imgs_A, imgs_B,\n",
    "                                                 imgs_A, imgs_B\n",
    "                                                 ]\n",
    "                                                )\n",
    "            # training updates every 50 iterations\n",
    "            if idx % 50 == 0:\n",
    "                print(f'[Epoch {idx}/{epoch}] '\n",
    "                        f'[Discriminator loss: {discriminator_loss[0]} Accuracy: {100 * discriminator_loss[1]:.2f}] '\n",
    "                        f'[Adversarial loss (A to B): {gen_loss[0]}] '\n",
    "                        f'[Adversarial loss (B to A): {gen_loss[1]}] '\n",
    "                        f'[Reconstruction loss (A): {gen_loss[2]}] '\n",
    "                        f'[Reconstruction loss (B): {gen_loss[3]}] '\n",
    "                        f'[Identity loss (A): {gen_loss[4]}] '\n",
    "                        f'[Identity loss (B): {gen_loss[5]}]')\n",
    "            \n",
    "            # plot and save progress every few iterations\n",
    "            if idx % sample_interval == 0:\n",
    "                plot_sample_images(gen_AtoB, \n",
    "                                   gen_BtoA,\n",
    "                                   path=path,\n",
    "                                   epoch = epoch,\n",
    "                                   batch_num= idx,\n",
    "                                   output_dir= f'outputs_{dataset_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainig(gen_AtoB, \n",
    "      gen_BtoA, \n",
    "      disc_A, \n",
    "      disc_B, \n",
    "      cycle_gan, \n",
    "      patch_gan_shape, \n",
    "      epochs=100,  #  50 epochs for appleorange dataset took 216 minutes\n",
    "      path = f'{data_directory}/{dataset_name}',\n",
    "      batch_size=1, \n",
    "      sample_interval=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
